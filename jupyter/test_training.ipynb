{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 20:19:01.504262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-30 20:19:02.662285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> UID of this run: 230730-201904-rTc2Gk\n",
      "run_number : 0\n",
      "Directory /import/home2/yhchenmath/Log/CellSeg/SCS/UNDEFINED/0 exists, please choose an option:\n",
      "Old files deleted.\n",
      "======> Directory for this run: /import/home2/yhchenmath/Log/CellSeg/SCS/UNDEFINED/0\n",
      "2023-07-30 20:20:00,118 INFO Opts: Namespace(dataset={'name': 'scs_mouse_brain', 'loc': 'SCS', 'patch_size': 1200, 'bin_size': 3, 'n_neighbor': 50, 'max_patch_num': 1}, model={'name': 'scs', 'use_amp': False, 'use_sam': False, 'arch': {'class_num': 16, 'input_shape': 2000, 'input_position_shape': 2, 'projection_dim': 64, 'num_heads': 1, 'transformer_units': 128, 'transformer_layers': 8, 'mlp_head_units': [1024, 256]}, 'optimizer': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0001}, 'schedulers': None}, training={'num_epochs': 1, 'batch_size': 10}, gpu='0', code_dir='./', data_dir='/import/home2/yhchenmath/Dataset/CellSeg', log_dir='/import/home2/yhchenmath/Log/CellSeg', exp_name='SCS', run_name='UNDEFINED', run_number='0', seed=0, option_for_existing_dir=None, load_json=True, json_path='./configs/mouse_adult_brain.json', mode='train', uid='230730-201904-rTc2Gk')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir('/import/home2/yhchenmath/Code/CellSegmentation/')\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.scs import dir_to_class\n",
    "import torch\n",
    "from options import Options, HParams\n",
    "from utils.experiman import ExperiMan\n",
    "from data import find_dataset_using_name\n",
    "\n",
    "manager = ExperiMan(name='default')\n",
    "parser = manager.get_basic_arg_parser()\n",
    "opt = Options(parser).parse()  # get training options\n",
    "\n",
    "manager.setup(opt)\n",
    "opt = HParams(**vars(opt))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-30 20:20:00,411 INFO ======> Single GPU Training\n",
      "|-----> Constructing count matrices.\n",
      "|-----> <insert> __type to uns in AnnData Object.\n",
      "|-----> <insert> pp to uns in AnnData Object.\n",
      "|-----> <insert> spatial to uns in AnnData Object.\n",
      "2023-07-30 20:21:29,657 INFO ======> 108 patches will be processed.\n",
      "|-----> <select> stain layer in AnnData Object\n",
      "|-----> <select> unspliced layer in AnnData Object\n",
      "|-----> Refining alignment in rigid mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss -1.5378e-02: 100%|████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----> Transforming layers ['stain']\n",
      "|-----> <select> stain layer in AnnData Object\n",
      "|-----> <insert> stain to layers in AnnData Object.\n",
      "|-----> <select> stain layer in AnnData Object\n",
      "|-----> Constructing nuclei mask from staining image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----> <insert> stain_mask to layers in AnnData Object.\n",
      "|-----> <select> stain_mask layer in AnnData Object\n",
      "|-----> Finding peaks with minimum distance 7.\n",
      "|-----> <insert> stain_distances to layers in AnnData Object.\n",
      "|-----> <insert> stain_markers to layers in AnnData Object.\n",
      "|-----> <select> stain layer in AnnData Object\n",
      "|-----> <select> stain_mask layer in AnnData Object\n",
      "|-----> <select> stain_markers layer in AnnData Object\n",
      "|-----> Running Watershed.\n",
      "|-----> <insert> watershed_labels to layers in AnnData Object.\n",
      "2023-07-30 20:26:27,761 INFO ======> dataset [SCSMouseBrainDataset] was created\n",
      "2023-07-30 20:26:27,763 INFO ======> length of dataset: 1\n",
      "2023-07-30 20:26:27,764 INFO ======> Model: scs\n",
      "2023-07-30 20:26:27,790 INFO ======> creating model\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n",
    "logger = manager.get_logger()\n",
    "logger.info(f\"======> Single GPU Training\")\n",
    "# Set up tensorboard\n",
    "manager._third_party_tools = ('tensorboard',)\n",
    "manager._setup_third_party_tools()\n",
    "\n",
    "# Create dataset\n",
    "dataset_all_patches = find_dataset_using_name(dataset_name=opt.dataset.name)(opt, manager)\n",
    "logger.info(\"======> dataset [%s] was created\" % type(dataset_all_patches).__name__)\n",
    "logger.info(f\"======> length of dataset: {len(dataset_all_patches)}\")\n",
    "\n",
    "# Method\n",
    "logger.info(f\"======> Model: \"+ opt.model.name)\n",
    "if opt.model.name == \"scs\":\n",
    "    from models.scs_model import SCSModel as Model\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "logger.info(f\"======> creating model\")\n",
    "model = Model(\n",
    "    opt = opt,\n",
    "    manager = manager,\n",
    "    dataloader = None\n",
    ").cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "per_patch_dataset = dataset_all_patches[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "per_patch_loader = DataLoader(\n",
    "    per_patch_dataset,\n",
    "    batch_size = opt.training.batch_size,\n",
    "    num_workers = 1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for x_train, x_train_pos, y_train, y_binary_train in per_patch_loader:\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 50, 2000]),\n torch.Size([10, 50, 2]),\n torch.Size([10, 2]),\n torch.Size([10]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_train_pos.shape, y_train.shape, y_binary_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_train_pos_ =x_train_pos\n",
    "for i in range(len(x_train_pos_)):\n",
    "    for j in range(1, len(x_train_pos_[i])):\n",
    "        x_train_pos_[i][j] = x_train_pos_[i][j] - x_train_pos_[i][0]\n",
    "    x_train_pos_[i][0] = x_train_pos_[i][0] - x_train_pos_[i][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "for i in range(len(y_train)):\n",
    "    if y_train[i][0] != -1:\n",
    "        y_train[i] = y_train[i] - x_train_pos[i][0]\n",
    "    else:\n",
    "        y_train[i][0] = -9999\n",
    "        y_train[i][1] = -9999\n",
    "y_train = dir_to_class(y_train, opt.model.arch.class_num)\n",
    "y_train = torch.from_numpy(y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 16])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-30 20:26:30,893 INFO iter 1 | {'loss.global': 2.0916177463531493, 'loss.pos': 1.391213400363922, 'loss.bi': 0.7004043459892273}\n"
     ]
    }
   ],
   "source": [
    "total_num_steps = 0\n",
    "losses, predictions = model.train_step({\n",
    "    'x': [x_train.float(), x_train_pos.float()],\n",
    "    'y': [y_train, y_binary_train.unsqueeze(1).float()]\n",
    "})\n",
    "total_num_steps += 1\n",
    "logger.info(f\"iter {total_num_steps} | {losses}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
